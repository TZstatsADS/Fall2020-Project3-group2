---
title: "Main"
author: "Tianle Zhu"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

In your final repo, there should be an R markdown file that organizes **all computational steps** for evaluating your proposed Facial Expression Recognition framework. 

This file is currently a template for running evaluation experiments. You should update it according to your codes but following precisely the same structure. 

```{r message=FALSE, warning=FALSE, include=FALSE}
if(!require("EBImage")){
  install.packages("BiocManager")
  BiocManager::install("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("ggplot2")){
  install.packages("ggplot2")
}

if(!require("caret")){
  install.packages("caret")
}

if(!require("glmnet")){
  install.packages("glmnet")
}

if(!require("WeightedROC")){
  install.packages("WeightedROC")
}

if(!require("gbm")){
  install.packages("gbm")
}

if(!require("mltools")){
  install.packages("mltools")
}

if(!require("ROSE")){
  install.packages("ROSE")
}

if(!require("e1071")){
  install.packages("e1071")
}

library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(glmnet)
library(WeightedROC)
library(gbm)
library(mltools)
library(ROSE)
library(e1071)
```

### Step 0 set work directories
```{r wkdir, eval=FALSE}
set.seed(2020)
```

Provide directories for training images. Training images and Training fiducial points will be in different subfolders. 
```{r}
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep= "")
train_pt_dir <- paste(train_dir,  "points/", sep= "")
train_label_path <- paste(train_dir, "label.csv", sep= "") 
```

## Part 1: Baseline Model  (GBM and logistic regression LASSO)

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (T/F) reweighting the samples for training set 
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv <- TRUE # run cross-validation on the training set
sample.reweight <- TRUE # run sample reweighting in model training
K <- 5  # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test <- TRUE # run evaluation on an independent test set
run.feature.test <- TRUE # process features for test set
```

By Using cross-validation, we want to use different specifications to compare the performance of models. We setup t as tune parameter for GBM. 

```{r model_setup}
k = c(50,100,150,200,250,300)
model_labels = paste("GBM with k =", k)
```

### Step 2: import data and train-test split 
```{r}
#train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)
```

If you choose to extract features from images, such as using Gabor filter, R memory will exhaust all images are read together. The solution is to repeat reading a smaller batch(e.g 100) and process them. 
```{r}
n_files <- length(list.files(train_image_dir))
image_list <- list()
for(i in 1:100){
   image_list[[i]] <- readImage(paste0(train_image_dir, sprintf("%04d", i), ".jpg"))
}
```

Fiducial points are stored in matlab format. In this step, we read them and store them in a list.
```{r read fiducial points}
# #function to read fiducial points
# #input: index
# #output: matrix of fiducial points corresponding to the index
# readMat.matrix <- function(index){
#      return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
# }
# #load fiducial points
# fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
# save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
```

```{r load fiducial points fiducial_pt_list}
load("../output/fiducial_pt_list.RData")
```


### Step 3: construct features and responses

+ The follow plots show how pairwise distance between fiducial points can work as feature for facial emotion recognition.

  + In the first column, 78 fiducials points of each emotion are marked in order. 
  + In the second column distributions of vertical distance between right pupil(1) and  right brow peak(21) are shown in  histograms. For example, the distance of an angry face tends to be shorter than that of a surprised face.
  + The third column is the distributions of vertical distances between right mouth corner(50)
and the midpoint of the upper lip(52).  For example, the distance of an happy face tends to be shorter than that of a sad face.

![Figure1](../figs/feature_visualization.jpg)

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
  
  + `feature.R`
  + Input: list of images or fiducial point
  + Output: an RData file that contains extracted features and corresponding responses

```{r feature}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
  save(dat_train, file="../output/feature_train.RData")
}else{
  load(file="../output/feature_train.RData")
}
tm_feature_test <- NA
if(run.feature.test){
  tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
  save(dat_test, file="../output/feature_test.RData")
}else{
  load(file="../output/feature_test.RData")
}
```

```{r}
load(file="../output/feature_test.RData")
load(file="../output/feature_train.RData")
```


## Part 2: Baseline Model  (SVM)

### Step 1 - 3: Same as the baseline model 

Same as Baseline model  

Using cross-validation, we compare the performance of models with different specifications. In the following chunk of code, we tune parameter cost_svm (number of shrinkage) for the Support vector machine (SVM)

```{r model_setup_2}
cost_svm = c(0.00001, 0.0001, 0.001, 0.01, 0.1)
model_labels_svm = paste("SVM with cost =", cost_svm) 
model_labels_svm
```

### Step 4: Train a classification model with training features and responses
```{r loadlib_2}
source("../lib/SVM_model.R")
```

``` {r svm run.cv, eval=FALSE}
# SVM Cross-validation
cost = c(0.00001, 0.0001, 0.001, 0.01, 0.1)
model_labels_svm = paste("SVM with cost =", cost) 
model_labels_svm

err_svm <- matrix(0, nrow = length(cost), ncol = 2)
for(i in 1:length(cost)){
   print(paste("cost=", cost[i]))
   err_svm[i,] <- CV_SVM(dat_train, K=5, cost[i])
   save(err_svm, file="../output/err_svm.RData")
 }
```

```{r svm cv visualization}
#Load visualization of cross validation results of svm
load("../output/err_svm.RData")
err_svm <- as.data.frame(err_svm)
colnames(err_svm) <- c("mean_error", "sd_error")
cost = c(0.00001, 0.0001, 0.001, 0.01, 0.1)
err_svm$cost = as.factor(cost)
err_svm %>% ggplot(aes(x = cost, y = mean_error, ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
    geom_crossbar() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Find the best cost for SVM model 
```{r svm best_model}
cost_best_svm <- cost[which.min(err_svm[,1])]
#par_best_svm <- list(cost = cost_best_svm)
# Training 
tm_train_svm = NA
tm_train_svm <- system.time(fit_train_svm <- svm(label ~., data = dat_train, kernel = "linear", cost = cost_best_svm) )
#Save and load model
saveRDS(fit_train_svm, "../output/fit_train_svm.RDS")
```

```{r}
fit_train_svm <- readRDS("../output/fit_train_svm.RDS")
# Testing
tm_test_svm=NA
tm_test_svm <- system.time(pred_svm <- predict(fit_train_svm, dat_test))
# Evaluation
accu_svm <- mean(dat_test$label == pred_svm)
real_label = dat_test$label %>% as.character() %>% as.numeric()
pred_value_svm  = pred_svm %>% as.character() %>% as.numeric()
confusionMatrix(pred_svm,dat_test$label)
cost_choose <- cost[which.min(err_svm[,1])]
cat("The accuracy of model: cost =", cost_choose, "is", accu_svm*100, "%.\n")

# The AUC for SVM model
AUC_SVM = auc_roc(real_label, pred_value_svm)
AUC_SVM 
```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited.


```{r running_time_2}
Model_performace <- function(time_feature_train, time_feature_test, time_train, time_test){
  cat("Time for constructing training features=", time_feature_train[1], "s \n")
  cat("Time for constructing testing features=", time_feature_test[1], "s \n")
  cat("Time for training model=", time_train[1], "s \n") 
  cat("Time for testing model=", time_test[1], "s \n")
} 
cat("The accuracy of the SVM model: cost =", cost[which.min(err_svm[,1])], "is", accu_svm*100, "%.\n")
cat("The auc value for the SVM model is",AUC_SVM * 100, "%.\n")
Model_performace(tm_feature_train, tm_feature_test, tm_train_svm, tm_test_svm)

```



## Part 2: Improved Model  (SVM)

### Step 1 - 2 : Same as Baseline SVM model 

  
### Step 3: construct features and responses  
Feature extraction is the same as Baseline model, and we improved the feature by using Principal Components Analysis for feature selection.  

`feature_improved.R` should be the wrapper for all your feature engineering functions and options. The function `feature_improved()` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later.  
  + `feature_improved.R`  
  + Input: train/test data  
  + Output: an RData file that contains extracted features and corresponding responses  
  
```{r feature_2}
source("../lib/feature_improved.R") # change file name
data_test_imp <- dat_test[ ,-ncol(dat_test)]
data_train_imp <- dat_train[ ,-ncol(dat_train)]

tm_feature_train_imp <- NA
if(run.feature.train){
  tm_feature_train_imp <- system.time(data_train_imp <- feature_improved(data_train_imp, index = NULL))
}

index_train_pca <- ncol(data_train_imp) 

tm_feature_test_improved <- NA
if(run.feature.test){
  tm_feature_test_imp <- system.time(data_test_imp <- feature_improved(data_test_imp, index = index_train_pca))
}

save(data_train_imp, file="../output/feature_train_imp.RData")
save(data_test_imp, file="../output/feature_test_imp.RData")

data_train_imp <- as.data.frame(data_train_imp)
data_train_imp$label <- dat_train$label
colnames(data_train_imp)

data_test_imp <- as.data.frame(data_test_imp)
data_test_imp$label <- dat_test$label
colnames(data_test_imp)
```

### Step 4: Train a classification model with training features and responses  

```{r}
source("../lib/SVM_model_weighted.R") 
```

``` {r svm_improved run.cv, eval=FALSE}
#SVM Cross-validation
cost = c(0.00001,0.0001,0.001,0.01,0.1,1)
err_svm_imp <- matrix(0, nrow = length(cost), ncol = 2)
for(i in 1:length(cost)){
   print(paste("cost:", cost[i]))
   err_svm_imp[i,] <- CV_SVM_weight(data_train_imp, K = 5, cost[i])
   saveRDS(err_svm_imp, file="../output/err_svm_imp.RDS")
}
err_svm_imp
cost_best_svm_imp <- cost[which.min(err_svm_imp[,1])]
cost_best_svm_imp
```

From `err_svm_imp`, we obtained that the cost didn't influence the error mean and the error's standard deviation 

```{r svm_imp best_model}
cost_best_svm_imp = 0.01
# Training
tm_train_svm_imp = NA
temp <- ovun.sample(label ~ ., data = data_train_imp, method = "over", p = 0.3)$data
tm_train_svm_imp <- system.time(fit_train_svm_imp <- svm(label ~., data = temp, kernel = "linear", cost = cost_best_svm_imp) )
#Save and load model
saveRDS(fit_train_svm_imp, "../output/fit_train_svm_imp.RDS")
```

```{r}
fit_train_svm_imp <- readRDS("../output/fit_train_svm_imp.RDS")
# Testing 
tm_test_svm_imp = NA
tm_test_svm_imp <- system.time(pred_svm_imp <- predict(fit_train_svm_imp, data_test_imp))

# Evaluation
accu_svm_imp <- mean(dat_test$label == pred_svm_imp)
accu_svm_imp 

pred_value_svm_imp  = pred_svm_imp %>% as.character() %>% as.numeric()
real_label = dat_test$label %>% as.character() %>% as.numeric()

confusionMatrix(pred_svm_imp, dat_test$label)

print(paste("The accuracy of model: cost =", cost_best_svm_imp, "is", accu_svm_imp * 100, "%"))

# The AUC for SVM model
AUC_SVM_imp = auc_roc(real_label, pred_value_svm_imp)
AUC_SVM_imp

```
  
### Summarize Running Time  
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited.  
```{r running_time_3}

print(paste("The accuracy of the SVM improved model: cost =", cost_best_svm_imp , "is", accu_svm_imp * 100, "%"))
print(paste("The auc value for tje SVM improved model is", AUC_SVM_imp * 100, "%"))
Model_performace(tm_feature_train_imp, tm_feature_test_imp, tm_train_svm_imp, tm_test_svm_imp)

```



## Part 3: Appendix -- Additional Models Tried (Random Forest, VGG16, XGboost, K Nearest Neighbors (KNN))

### VGG16

### Random Forest

### XGboost

### K Nearest Neighbors (KNN)




###Reference
- Du, S., Tao, Y., & Martinez, A. M. (2014). Compound facial expressions of emotion. Proceedings of the National Academy of Sciences, 111(15), E1454-E1462.













